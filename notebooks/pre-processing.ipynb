{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline de Pr√© Processamento para NLP\n",
    "\n",
    "# Pr√© Processamento de texto\n",
    "\n",
    "Para pipelines de pr√© processamento baseadadas em compreens√£o de texto, √© crucial que fa√ßamos uma normaliza√ß√£o e limpeza dos dados antes de analisar-los. Esse pr√© processamento envolve algumas etapas:\n",
    "\n",
    "- **Tokeniza√ß√£o:** Separar o texto em palavras comuns ou tokens. Por exemplo, \"Ol√° mundo!\" ir√° virar [\"ola\", \"mundo\", \"!\"].\n",
    "- **Lowercasing:** Converte todos os caracteres em textos min√∫sculos para garantir uniformidade. Por exemplo, \"Ola\" e \"ola\" s√£o tratados igualmente.\n",
    "- **Remo√ß√£o de Pontua√ß√£o:** Eliminar marcas de pontua√ß√£o j√° que elas n√£o contribuem com o significado das palavras.\n",
    "- **Remo√ß√£o de Stop Words:** Remo√ß√£o de palavras comuns como \"e\", \"o\", \"√©\", que n√£o carregam um significado.\n",
    "- **Lematiza√ß√£o:** Convertendo palavras para a sua forma base. Por exemplo, \"correndo\" ir√° virar \"correr\".\n",
    "- **Stemming:** Semelhante a lematiza√ß√£o, mas geralmente mais agressiva. Ela reduz as palavras para a sua forma ra√≠z, por exemplo \"jogando\" √© reduzido para \"jog\", que √© a raiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../data/IMDB_Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padronizando os caracteres para lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"one of the other reviewers has mentioned that after watching just 1 oz episode you'll be hooked. they are right, as this is exactly what happened with me.<br /><br />the first thing that struck me about oz was its brutality and unflinching scenes of violence, which set in right from the word go. trust me, this is not a show for the faint hearted or timid. this show pulls no punches with regards to drugs, sex or violence. its is hardcore, in the classic use of the word.<br /><br />it is called oz as that is the nickname given to the oswald maximum security state penitentary. it focuses mainly on emerald city, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. em city is home to many..aryans, muslims, gangstas, latinos, christians, italians, irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />i would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. forget pretty pictures painted for mainstream audiences, forget charm, forget romance...oz doesn't mess around. the first episode i ever saw struck me as so nasty it was surreal, i couldn't say i was ready for it, but as i watched more, i developed a taste for oz, and got accustomed to the high levels of graphic violence. not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) watching oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. <br /><br />the...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review']=df['review'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removendo Tags HTML utilizando express√µes regulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. the filming tec...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review']=df['review'].apply(remove_html_tags)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removendo URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Checkout this webpage to learn more about deep learning '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_url(text):\n",
    "    pattern=re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'',text)\n",
    "\n",
    "text='Checkout this webpage to learn more about deep learning https://www.deeplearning.ai/ai-notes/initialization/index.html'\n",
    "remove_url(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removendo Pontua√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string,time\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'okay so this is just a text with punc'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude=string.punctuation\n",
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text=text.replace(char,'')\n",
    "    return text\n",
    "\n",
    "def remove_punc1(text):\n",
    "    return text.translate(str.maketrans('','',exclude))\n",
    "\n",
    "text='okay, so. this, is/ just a, text, with. punc'\n",
    "remove_punc(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamento de linguagem Gera√ß√£o Z\n",
    "Para os modelos performarem melhor durante suas interpreta√ß√µes, precisamos que ele compreenda o significado por tr√°s das abrevia√ß√µes feitas na internet, infelizmente significando que temos que ensinar aos modelos um pouco da linguagem da gera√ß√£o Z do Twitter :,(.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_word = {\n",
    "    'AFAIK': 'As Far As I Know',\n",
    "    'AFK': 'Away From Keyboard',\n",
    "    'ASAP': 'As Soon As Possible',\n",
    "    'ATK': 'At The Keyboard',\n",
    "    'ATM': 'At The Moment',\n",
    "    'A3': 'Anytime, Anywhere, Anyplace',\n",
    "    'BAK': 'Back At Keyboard',\n",
    "    'BBL': 'Be Back Later',\n",
    "    'BBS': 'Be Back Soon',\n",
    "    'BFN': 'Bye For Now',\n",
    "    'B4N': 'Bye For Now',\n",
    "    'BRB': 'Be Right Back',\n",
    "    'BRT': 'Be Right There',\n",
    "    'BTW': 'By The Way',\n",
    "    'B4': 'Before',\n",
    "    'CU': 'See You',\n",
    "    'CUL8R': 'See You Later',\n",
    "    'CYA': 'See You',\n",
    "    'FAQ': 'Frequently Asked Questions',\n",
    "    'FC': 'Fingers Crossed',\n",
    "    'FWIW': \"For What It's Worth\",\n",
    "    'FYI': 'For Your Information',\n",
    "    'GAL': 'Get A Life',\n",
    "    'GG': 'Good Game',\n",
    "    'GN': 'Good Night',\n",
    "    'GMTA': 'Great Minds Think Alike',\n",
    "    'GR8': 'Great!',\n",
    "    'G9': 'Genius',\n",
    "    'IC': 'I See',\n",
    "    'ICQ': 'I Seek you (also a chat program)',\n",
    "    'ILU': 'ILU: I Love You',\n",
    "    'IMHO': 'In My Honest/Humble Opinion',\n",
    "    'IMO': 'In My Opinion',\n",
    "    'IOW': 'In Other Words',\n",
    "    'IRL': 'In Real Life',\n",
    "    'KISS': 'Keep It Simple, Stupid',\n",
    "    'LDR': 'Long Distance Relationship',\n",
    "    'LMAO': 'Laugh My A.. Off',\n",
    "    'LOL': 'Laughing Out Loud',\n",
    "    'LTNS': 'Long Time No See',\n",
    "    'L8R': 'Later',\n",
    "    'MTE': 'My Thoughts Exactly',\n",
    "    'M8': 'Mate',\n",
    "    'NRN': 'No Reply Necessary',\n",
    "    'OIC': 'Oh I See',\n",
    "    'PITA': 'Pain In The A..',\n",
    "    'PRT': 'Party',\n",
    "    'PRW': 'Parents Are Watching',\n",
    "    'QPSA?': 'Que Pasa?',\n",
    "    'ROFL': 'Rolling On The Floor Laughing',\n",
    "    'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n",
    "    'ROTFLMAO': 'Rolling On The Floor Laughing My A.. Off',\n",
    "    'SK8': 'Skate',\n",
    "    'STATS': 'Your sex and age',\n",
    "    'ASL': 'Age, Sex, Location',\n",
    "    'THX': 'Thank You',\n",
    "    'TTFN': 'Ta-Ta For Now!',\n",
    "    'TTYL': 'Talk To You Later',\n",
    "    'U': 'You',\n",
    "    'U2': 'You Too',\n",
    "    'U4E': 'Yours For Ever',\n",
    "    'WB': 'Welcome Back',\n",
    "    'WTF': 'What The F...',\n",
    "    'WTG': 'Way To Go!',\n",
    "    'WUF': 'Where Are You From?',\n",
    "    'W8': 'Wait...',\n",
    "    '7K': 'Sick:-D Laugher',\n",
    "    'TFW': 'That feeling when',\n",
    "    'MFW': 'My face when',\n",
    "    'MRW': 'My reaction when',\n",
    "    'IFYP': 'I feel your pain',\n",
    "    'TNTL': 'Trying not to laugh',\n",
    "    'JK': 'Just kidding',\n",
    "    'IDC': \"I don't care\",\n",
    "    'ILY': 'I love you',\n",
    "    'IMU': 'I miss you',\n",
    "    'ADIH': 'Another day in hell',\n",
    "    'ZZZ': 'Sleeping, bored, tired',\n",
    "    'WYWH': 'Wish you were here',\n",
    "    'TIME': 'Tears in my eyes',\n",
    "    'BAE': 'Before anyone else',\n",
    "    'FIMH': 'Forever in my heart',\n",
    "    'BSAAW': 'Big smile and a wink',\n",
    "    'BWL': 'Bursting with laughter',\n",
    "    'BFF': 'Best friends forever',\n",
    "    'CSL': \"Can't stop laughing\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'As Soon As Possible let me know please'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def short_conv(text):\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_word:\n",
    "            new_text.append(chat_word[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "\n",
    "short_conv(\"ASAP let me know please\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corre√ß√£o Ortogr√°fica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' certainly I dont know what is wrong here'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text=\" ceertainli I dont kniw what is wrrong herre\"\n",
    "textblb=TextBlob(text)\n",
    "textblb.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remo√ß√£o de Stop Words\n",
    "Stop words s√£o palavras comuns em uma linguagem que muitas vezes s√£o removidas porque carregam pouco peso sem√¢ntico e podem desordenar a an√°lise. Essas palavras incluem artigos (por exemplo, \"a\", \"the\"), conjun√ß√µes (por exemplo, \"e\", \"ou\"), preposi√ß√µes (por exemplo, \"in\", \"on\") e outros termos que ocorrem com freq√º√™ncia que n√£o contribuem significativamente para o significado de uma frase. Ao remover palavras de parada, o foco muda para palavras mais significativas que representam melhor o conte√∫do do texto, melhorando a efici√™ncia e a precis√£o da an√°lise de texto e dos modelos de aprendizado de m√°quina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Thiago\n",
      "[nltk_data]     Barros\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I  sure   might happened'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text=[]\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "            \n",
    "    x=new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)\n",
    "\n",
    "text=\"I wasn't sure that this might happened\"\n",
    "remove_stopwords(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamento de Emojis\n",
    "Aqui temos duas formas de tratar os emojis, sendo removendo-os completamente ou apenas substituindo eles pelo o seu valor sem√¢ntico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'that is not so funny please stop '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removendo os emojis\n",
    "import re\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern=re.compile(\"[\"\n",
    "                             u\"\\U0001F600-\\U0001F64F\" #emoticons\n",
    "                             u\"\\U0001F300-\\U0001F5FF\" #symbols, pictograph\n",
    "                              u\"\\U0001F680-\\U0001F6FF\" #transport and map symbol\n",
    "                              u\"\\U0001F1E0-\\U0001F1FF\" # flags(IOS)\n",
    "                              u\"\\U00002702-\\U000027B0\"\n",
    "                              u\"\\U00002FC2-\\U0001F251\"\n",
    "                             \"]+\",flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'',text)\n",
    "\n",
    "text=\"that is not so funny please stop üò≠\"\n",
    "remove_emoji(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that is not so funny please stop :loudly_crying_face:\n"
     ]
    }
   ],
   "source": [
    "# Substituindo\n",
    "import emoji\n",
    "print(emoji.demojize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokeniza√ß√£o\n",
    "A tokeniza√ß√£o √© o passo mais fundamental na NLP que envolve dividir o texto em unidades menores chamadas tokens, que podem ser palavras, frases ou frases. Esse processo ajuda a converter o texto bruto em um formato estruturado para an√°lise. Existem diferentes tipos de tokeniza√ß√£o, incluindo tokeniza√ß√£o de palavras (dividir texto em palavras individuais) e tokeniza√ß√£o de frases (dividir texto em frases). A tokeniza√ß√£o adequada considera a pontua√ß√£o, os espa√ßos e as regras espec√≠ficas do idioma para representar com precis√£o a estrutura e o significado do texto, permitindo um processamento de texto e extra√ß√£o de recursos mais eficazes nas tarefas de NLP.\n",
    "\n",
    "Veremos quatro formas de fazer essa tokeniza√ß√£o: \n",
    "- Via Python split\n",
    "- Utilizando express√µes regulares\n",
    "- Usando a biblioteca NLTK\n",
    "- Usando a biblioteca Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usando a fun√ß√£o de split\n",
    "sent1=\"I am going to delhi\"\n",
    "sent1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am', ' going to delhi']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usando a fun√ß√£o de split\n",
    "sent1=\"I am, going to delhi\"\n",
    "sent1.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am', ' going to delhi!!!']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# O problema que surge, ele n√£o consegue separar os '!!!!' \n",
    "sent1=\"I am, going to delhi!!!\"\n",
    "sent1.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\w'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\w'\n",
      "C:\\Users\\Thiago Barros\\AppData\\Local\\Temp\\ipykernel_3472\\4056234187.py:3: SyntaxWarning: invalid escape sequence '\\w'\n",
      "  tokens=re.findall(\"[\\w']+\",sent1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utilizando express√µes regulares\n",
    "import re\n",
    "tokens=re.findall(\"[\\w']+\",sent1)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Thiago\n",
      "[nltk_data]     Barros\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I', 'am', ',', 'going', 'to', 'delhi', '!', '!', '!']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Utilizando a biblioteca NLTK\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2='I have a Ph.D in M.L'\n",
    "sent3=\"We're here to help! mail us at xuz@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'have', 'a', 'Ph.D', 'in', 'M.L']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " \"'re\",\n",
       " 'here',\n",
       " 'to',\n",
       " 'help',\n",
       " '!',\n",
       " 'mail',\n",
       " 'us',\n",
       " 'at',\n",
       " 'xuz',\n",
       " '@',\n",
       " 'gmail.com']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# falha nessa senten√ßa, pois separou o id do e-mail no b√°sico de @\n",
    "word_tokenize(sent3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 27.3 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 21.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.6/12.8 MB 23.9 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 23.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\thiago barros\\documents\\github\\twitter-sentiment-analysis\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "have\n",
      "a\n",
      "Ph\n",
      ".\n",
      "D\n",
      "in\n",
      "M.L\n"
     ]
    }
   ],
   "source": [
    "# Utilizando o Spacy\n",
    "import spacy\n",
    "\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "doc1=nlp(sent1)\n",
    "doc2=nlp(sent2)\n",
    "doc3=nlp(sent3)\n",
    "\n",
    "for token in doc2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Stemming reduz as palavras √† sua raiz ou forma de base, removendo sufixos. Ao contr√°rio da lematiza√ß√£o, que se baseia em regras lingu√≠sticas, o stemming usa m√©todos heur√≠sticos para remover afixos. Por exemplo, \"running\", \"runner,\" e \"ran\" podem ser reduzidos a \"run.\" Algoritmos comuns incluem o Porter Stemmer, que aplica uma s√©rie de regras a sufixos iterativos, e o Snowball Stemmer, que √© uma vers√£o melhorada do Porter Stemmer. Embora o stemming seja mais r√°pido e mais simples do que a lematiza√ß√£o, ele pode ser menos preciso, √†s vezes produzindo hastes que n√£o s√£o palavras reais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps=PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])\n",
    "\n",
    "sample=\"Walk walking walked walks\"\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stem is a text preprocess techniqu in nlp that reduc word to their root or base form by remov suffixes. unlik lemmatization, which reli on linguist rules, stem use heurist method to strip affixes. for example, \"running,\" \"runner,\" and \"ran\" might all be reduc to \"run.\" common algorithm includ the porter stemmer, which appli a seri of rule to iter strip suffixes, and the snowbal stemmer, which is an improv version of the porter stemmer. while stem is faster and simpler than lemmatization, it can be less accurate, sometim produc stem that are not actual words.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample2='Stemming is a text preprocessing technique in NLP that reduces words to their root or base form by removing suffixes. Unlike lemmatization, which relies on linguistic rules, stemming uses heuristic methods to strip affixes. For example, \"running,\" \"runner,\" and \"ran\" might all be reduced to \"run.\" Common algorithms include the Porter Stemmer, which applies a series of rules to iteratively strip suffixes, and the Snowball Stemmer, which is an improved version of the Porter Stemmer. While stemming is faster and simpler than lemmatization, it can be less accurate, sometimes producing stems that are not actual words.'\n",
    "stem_words(sample2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematiza√ß√£o\n",
    "\n",
    "A lematiza√ß√£o reduz as palavras √† sua forma base ou raiz, conhecida como lema. Ao contr√°rio da deriva√ß√£o, que simplesmente corta as termina√ß√µes das palavras, a lematiza√ß√£o considera o contexto e a an√°lise morfol√≥gica das palavras, garantindo que as palavras sejam transformadas em formas base significativas. Por exemplo, \"running\" se torna \"run\" e \"better\" se torna \"good\". Esse processo ajuda a padronizar palavras, melhorando a precis√£o da an√°lise de texto ao agrupar diferentes formas flexionadas de uma palavra em um √∫nico item. A lematiza√ß√£o √© essencial para tarefas como classifica√ß√£o de texto, an√°lise de sentimento e recupera√ß√£o de informa√ß√µes, pois melhora a qualidade dos recursos extra√≠dos do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Word - Lemmatized Word\n",
      "The - the\n",
      "children - child\n",
      "were - be\n",
      "playing - play\n",
      "in - in\n",
      "the - the\n",
      "park - park\n",
      ", - ,\n",
      "running - run\n",
      "and - and\n",
      "laughing - laugh\n",
      "as - as\n",
      "they - they\n",
      "enjoyed - enjoy\n",
      "their - their\n",
      "freedom - freedom\n",
      ", - ,\n",
      "unaware - unaware\n",
      "of - of\n",
      "the - the\n",
      "time - time\n",
      "passing - pass\n",
      "quickly - quickly\n",
      "by - by\n",
      ". - .\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sentence = \"The children were playing in the park, running and laughing as they enjoyed their freedom, unaware of the time passing quickly by.\"\n",
    "doc = nlp(sentence)\n",
    "\n",
    "print(\"Original Word - Lemmatized Word\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text} - {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot Encoding\n",
    "One Hot Encoding √© uma t√©cnica usada para converter dados categ√≥ricos em um formato num√©rico que pode ser usado por algoritmos de aprendizado de m√°quina. Este m√©todo transforma cada valor de categoria em uma nova coluna bin√°ria e atribui 1 ou 0 (Verdadeiro/Falso) para indicar a presen√ßa daquela categoria nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   ID  Color\n",
      "0   1    Red\n",
      "1   2  Green\n",
      "2   3   Blue\n",
      "3   4  Green\n",
      "4   5   Blue\n",
      "\n",
      "One-Hot Encoded DataFrame:\n",
      "   ID  Color  Color_Blue  Color_Green  Color_Red\n",
      "0   1    Red         0.0          0.0        1.0\n",
      "1   2  Green         0.0          1.0        0.0\n",
      "2   3   Blue         1.0          0.0        0.0\n",
      "3   4  Green         0.0          1.0        0.0\n",
      "4   5   Blue         1.0          0.0        0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "data = {\n",
    "    'ID': [1, 2, 3, 4, 5],\n",
    "    'Color': ['Red', 'Green', 'Blue', 'Green', 'Blue']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "encoded_data = encoder.fit_transform(df[['Color']])\n",
    "encoded_df = pd.DataFrame(encoded_data.toarray(), columns=encoder.get_feature_names_out(['Color']))\n",
    "final_df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\nOne-Hot Encoded DataFrame:\")\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words\n",
    "A intui√ß√£o central por tr√°s do modelo Bag of Words (BoW) √© que se nosso algoritmo detectar palavras semelhantes ocorrendo com frequ√™ncias semelhantes, ele classificar√° os documentos como estando na mesma classe. Aqui, a ordem das palavras e o contexto n√£o importam; o que importa √© a presen√ßa e a frequ√™ncia das palavras.\n",
    "\n",
    "No BoW, cada palavra √© representada como uma coordenada em um espa√ßo n-dimensional. Se definirmos o par√¢metro bin√°rio como True, qualquer palavra que apare√ßa mais de uma vez ainda ser√° tratada como se aparecesse apenas uma vez. Consequentemente, as coordenadas ser√£o bin√°rias, preenchidas apenas com 0s e 1s.\n",
    "\n",
    "#### Filtrando palavras raras\n",
    "Para remover palavras raras, voc√™ pode usar o par√¢metro max_features, que limita o vocabul√°rio √†s palavras mais frequentes. Por exemplo, definir max_features=1/2 ignora todas as palavras com frequ√™ncias menores que esse limite.\n",
    "\n",
    "#### Vantagens do Bag of Words\n",
    "Simplicidade e Intuitividade: F√°cil de implementar e entender.\n",
    "Mitiga√ß√£o de Problemas de Fora do Vocabul√°rio (OOV): Como o modelo usa um vocabul√°rio fixo, ele evita o problema de OOV que afeta a codifica√ß√£o one-hot.\n",
    "\n",
    "#### Desvantagens do Bag of Words\n",
    "\n",
    "- **Esparsidade:** Com um vocabul√°rio grande, a representa√ß√£o ter√° muitos 0s, potencialmente levando a overfitting.\n",
    "Perda de Informa√ß√£o: Ignora a ordem das palavras e o contexto, levando √† perda de significado sem√¢ntico.\n",
    "Exemplo: \"Este √© um filme muito bom\" e \"Este n√£o √© um filme muito bom\" podem ser representados de forma semelhante, o que pode causar problemas com similaridade de cosseno.\n",
    "\n",
    "#### Similaridade de cosseno\n",
    "A similaridade de cosseno mede o cosseno do √¢ngulo entre dois vetores, fornecendo uma m√©trica para sua similaridade. A f√≥rmula √©:\n",
    "\n",
    "#TODO: colocar a f√≥rmula aqui.\n",
    "\n",
    "Usando similaridade de cosseno, \"Este √© um filme muito bom\" e \"Este n√£o √© um filme muito bom\" podem parecer semelhantes devido ao √¢ngulo pr√≥ximo entre suas representa√ß√µes vetoriais no modelo BoW.\n",
    "\n",
    "Ao usar o BoW, voc√™ pode converter dados de texto em recursos num√©ricos para algoritmos de aprendizado de m√°quina de forma r√°pida e eficiente. No entanto, esteja ciente de suas limita√ß√µes e considere t√©cnicas mais avan√ßadas como TF-IDF, incorpora√ß√µes de palavras ou modelos de linguagem para capturar informa√ß√µes textuais mais matizadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Text</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1</td>\n",
       "      <td>People Follow Mr.Beast</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D2</td>\n",
       "      <td>Mr.Beast Follow Mr.Beast</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D3</td>\n",
       "      <td>People Write comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D4</td>\n",
       "      <td>Mr.Beast Write comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Index                      Text  Output\n",
       "0    D1    People Follow Mr.Beast       1\n",
       "1    D2  Mr.Beast Follow Mr.Beast       1\n",
       "2    D3      People Write comment       0\n",
       "3    D4    Mr.Beast Write comment       0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Definindo o dataset\n",
    "data = {\n",
    "    'Index': ['D1', 'D2', 'D3', 'D4'],\n",
    "    'Text': [\n",
    "        'People Follow Mr.Beast',\n",
    "        'Mr.Beast Follow Mr.Beast',\n",
    "        'People Write comment',\n",
    "        'Mr.Beast Write comment'\n",
    "    ],\n",
    "    'Output': [1, 1, 0, 0]\n",
    "}\n",
    "\n",
    "# Criando um dataframe\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beast' 'comment' 'follow' 'mr' 'people' 'write']\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['Text'])\n",
    "\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people': 4, 'follow': 2, 'mr': 3, 'beast': 0, 'write': 5, 'comment': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Original:\n",
      "  Index                      Text  Output\n",
      "0    D1    People Follow Mr.Beast       1\n",
      "1    D2  Mr.Beast Follow Mr.Beast       1\n",
      "2    D3      People Write comment       0\n",
      "3    D4    Mr.Beast Write comment       0\n",
      "\n",
      "Bag of Words DataFrame com os par√¢metros bin√°rios como False:\n",
      "  Index  Output  beast  comment  follow  mr  people  write\n",
      "0    D1       1      1        0       1   1       1      0\n",
      "1    D2       1      1        0       1   1       0      0\n",
      "2    D3       0      0        1       0   0       1      1\n",
      "3    D4       0      1        1       0   1       0      1\n"
     ]
    }
   ],
   "source": [
    "bow_df = pd.DataFrame(X.toarray(), columns=vocab)\n",
    "\n",
    "final_df = pd.concat([df[['Index', 'Output']], bow_df], axis=1)\n",
    "\n",
    "print(\"DataFrame Original:\")\n",
    "print(df)\n",
    "print(\"\\nBag of Words DataFrame com os par√¢metros bin√°rios como False:\")\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe original:\n",
      "  Index                      Text  Output\n",
      "0    D1    People Follow Mr.Beast       1\n",
      "1    D2  Mr.Beast Follow Mr.Beast       1\n",
      "2    D3      People Write comment       0\n",
      "3    D4    Mr.Beast Write comment       0\n",
      "\n",
      "Bag of Words Dataframe com os par√¢metros bin√°rios como True:\n",
      "  Index  Output  beast  comment  follow  mr  people  write\n",
      "0    D1       1      1        0       1   1       1      0\n",
      "1    D2       1      1        0       1   1       0      0\n",
      "2    D3       0      0        1       0   0       1      1\n",
      "3    D4       0      1        1       0   1       0      1\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary=True)\n",
    "X = vectorizer.fit_transform(df['Text'])\n",
    "\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=vocab)\n",
    "\n",
    "final_df = pd.concat([df[['Index', 'Output']], bow_df], axis=1)\n",
    "\n",
    "print(\"Dataframe original:\")\n",
    "print(df)\n",
    "print(\"\\nBag of Words Dataframe com os par√¢metros bin√°rios como True:\")\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Para fazer\n",
    "## Testar mais m√©todos de pr√©-processamento:\n",
    "- N-Grams\n",
    "- TD-IDF\n",
    "- WordNet\n",
    "- Word2Vec\n",
    "- RoPE (Robust Positional Embeddings)\n",
    "\n",
    "## Automatizar a escolha e an√°lise do modelo\n",
    "- AutoSKlearn\n",
    "- TPOT\n",
    "- SMAC (sequential model-based algorithm configuration)\n",
    "\n",
    "## Pegar os melhores modelos e fazer a escolha dos hiperpar√¢metros\n",
    "- Utilizar m√©todos de GridSearch e XGBoost\n",
    "\n",
    "## Salvar o melhor modelo dentro de um padr√£o de versionamento\n",
    "- Salvar o modelo compilado para que seja utilizado em uma API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
